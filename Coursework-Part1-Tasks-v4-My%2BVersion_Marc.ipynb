{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Coursework Part 1: Detecting Spam with Spark\n",
    "\n",
    "IN432 Big Data coursework 2017, part 1. Classifying messages to detect spam. \n",
    "\n",
    "The overall goal is to transform the data into so that we can build a classifier. \n",
    "Then some aspects of the data and it's preparation will be explored. We will \n",
    "specifically study the effect of \n",
    "* the size of training set \n",
    "* the size of the representation vector, and \n",
    "* the preprocessing with stopword removal and/or lemmatisation.\n",
    "\n",
    "The team task also addresses the creating of TF.IDF vectors.\n",
    "\n",
    "Please add comments aobut the your findings at the end. \n",
    "\n",
    "> There is a new bit of code at the end of this file that you can use in case of \"sc undefined\" errors. This tries to create a new SparContext. After using it you will need to run all code cells again from the beginning.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task a) & b) Read some files and prepare a (f,w) RDD \n",
    "a) Start by reading the directory with text files from the distributed file system (e.g. `hdfs://saltdean.nsqdc.city.ac.uk./data/spam/bare/part1`), and loading all text files using wholeTextFiles(), which loads the text per file, i.e. tuples (f,t). (5%)\n",
    "\n",
    "> Note: while HDFS is not available, please use the files in `\\data\\tempstore\\`. A prefix mechanism has been added to this version to make it easy to switch. \n",
    "\n",
    "b) Split the text into words (lower case), creating a (file,word) RDD. (10%)\n",
    "\n",
    "For both tasks you can use the code from the labs. Don't remove finals 's' (we have already lemmatised data to work with later). \n",
    "\n",
    "It is very useful to package the code into a function that takes a directory as an argument and returns an RDD with (f,w) structure, e.g. `read_fw_RDD`.\n",
    "\n",
    "Please write two lines of code at the end of the cell that run a little example and print some output. This is for you to test your code during developemnt and for us to mark your work. You can comment them out after you have verified that your code works. I have left some of mine there, feel free to use those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read /data/tempstore/spam/bare/part1 files from directory 289\n",
      "file word count histogram\n",
      "([0, 10, 100, 1000, 10000], [0, 0, 107, 177])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('file:/data/tempstore/spam/bare/part1/3-437msg3.txt', 'subject'),\n",
       " ('file:/data/tempstore/spam/bare/part1/3-437msg3.txt', 'becoming'),\n",
       " ('file:/data/tempstore/spam/bare/part1/3-437msg3.txt', 'a')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "import re \n",
    "# USE DEPENDING ON DATASTORE\n",
    "#prefix = '/data/tempstore/'\n",
    "#prefix = 'hdfs://saltdean.nsqdc.city.ac.uk/data/'\n",
    "prefix  = '/data/tempstore/'\n",
    "\n",
    "dirPath = prefix + 'spam/bare/part1'\n",
    "\n",
    "\n",
    "\n",
    "def splitFileWords(filenameContent): # your splitting function\n",
    "    f,c = filenameContent # split the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+',c) # <<< now create a word list wLst\n",
    "    \n",
    "    for w in wLst: # iterate through the list\n",
    "        fwLst.append((f,w.lower())) # <<< and append (f,w) to the \n",
    "        \n",
    "    return fwLst #return a list of (f,w) tuples \n",
    "\n",
    "\n",
    "\n",
    "def read_fw_RDD(dirPath): # package tasks a/b into a function for later use\n",
    "#     # ... #<<< task a) read the files\n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) #<<< add code to create an RDD with wholeTextFiles\n",
    "    \n",
    "    print('Read {} files from directory {}'.format(dirPath, ft_RDD.count())) # status message for testing, can be disabled later on\n",
    "\n",
    "    print('file word count histogram') # the histogram can be useful for checking later \n",
    "    print(ft_RDD.map(lambda fwL: (len(fwL[1]))).histogram([0,10,100,1000,10000]))\n",
    "\n",
    "\n",
    "#     ... #<<< task b) split words\n",
    "    fw_RDD = ft_RDD.flatMap(splitFileWords)\n",
    "    return fw_RDD # A fw_RDD should be returned\n",
    "\n",
    "fw_RDD = read_fw_RDD(dirPath) # for testing\n",
    "fw_RDD.take(3) # for testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# words = text.flatMap(lambda x: re.split('\\W+',x)) # split words, break lists\n",
    "# words1 = words.map(lambda x: (stripFinalS(x),1)) # lower case, (w,1) pairs\n",
    "# wordCount = words1.reduceByKey(add) # reduce and add up counts\n",
    "# stopwlst = ['the','a','in','of','on','at','for','by','I','you','me'] # stopword list\n",
    "# freqWords = wordCount.filter(lambda x:  x[1] >= 5 or x in stopwlst ) # remove rare words\n",
    "# stopWords = freqWords.filter(lambda x:  x[0] in stopwlst ) # keep only stopwords\n",
    "# output = stopWords.collect() # collect results\n",
    "# for (word, count) in output: # iterate over (w,c) pairs\n",
    "#     print(\"%s: %i\" % (word, count)) #  … and print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task c) Normalised word count lists\n",
    "Use the code from the labs to generate the `[(word,count), ...]` list per file and to create a word frequency vector. \n",
    "\n",
    "Normalise the term frequency (TF) vector by the total word count per file. (15%)\n",
    "\n",
    "For this you can mostly reuse the lab code. The interesting part here is the normalisation. For normalisation we need to total word count per file. You can use a nested list comprehension for this (go through the (w,c) list and divide each c by the sum of all c, which you can get with a list). Alternatively, you can write a function where you can create local variables, e.g. for the number of words per file.  \n",
    "\n",
    "Another option is to use a separate RDD with (f,twc), where 'twc' is for total word count, and which you can create from the (f,[(w,c), ... ]) RDD. \n",
    "\n",
    "This new RDD can then be joined with the (f,[(w,c), ... ]) RDD and then the (w,c) list be normalised in a list comprehension. \n",
    "\n",
    "Again, put your code into a function, and add a short test that can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read /data/tempstore/spam/bare/part1 files from directory 289\n",
      "file word count histogram\n",
      "([0, 10, 100, 1000, 10000], [0, 0, 107, 177])\n",
      "[('file:/data/tempstore/spam/bare/part1/spmsga129.txt', ([('please', 1), ('99', 1), ('3', 1), ('a', 3), ('surrey', 1), ('fresh', 1), ('pound', 2), ('supplied', 1), ('po', 1), ('the', 13), ('59', 1), ('about', 1), ('net', 1), ('get', 1), ('post', 2), ('international', 1), ('152', 1), ('80', 1), ('dispatched', 1), ('discs', 1), ('date', 1), ('text', 1), ('box', 1), ('137', 1), ('then', 1), ('5', 2), ('120', 1), ('packing', 1), ('sites', 1), ('15', 1), ('lists', 3), ('how', 1), ('92', 1), ('email', 15), ('either', 1), ('who', 1), ('made', 1), ('data', 1), ('and', 11), ('you', 9), ('enclose', 1), ('number', 1), ('fully', 1), ('address', 3), ('come', 2), ('42', 1), ('it', 1), ('packages', 1), ('178', 1), ('cleared', 1), ('by', 2), ('unsolicited', 1), ('2', 1), ('direct', 1), ('exchange', 1), ('code', 1), ('expensive', 1), ('dollars', 3), ('that', 1), ('payable', 1), ('with', 5), ('gu15', 1), ('dollar', 1), ('they', 1), ('much', 1), ('receive', 1), ('most', 2), ('7', 1), ('uk', 2), ('should', 1), ('from', 2), ('not', 3), ('has', 1), ('or', 2), ('inclusive', 1), ('your', 8), ('software', 2), ('wish', 3), ('sterling', 2), ('minimum', 1), ('all', 2), ('worldwide', 1), ('75', 1), ('6', 1), ('soon', 1), ('what', 2), ('web', 1), ('so', 1), ('available', 1), ('bases', 1), ('bulk', 2), ('outside', 1), ('recieve', 1), ('doman', 1), ('products', 1), ('will', 3), ('cleaned', 1), ('dont', 2), ('travellers', 1), ('can', 1), ('postage', 1), ('for', 3), ('prices', 2), ('program', 1), ('word', 1), ('bypass', 1), ('need', 2), ('if', 3), ('order', 4), ('puchased', 1), ('35', 1), ('irritation', 1), ('cheques', 3), ('processing', 1), ('cheque', 1), ('together', 1), ('currency', 5), ('mail', 3), ('on', 2), ('its', 1), ('many', 1), ('recipient', 1), ('those', 1), ('be', 3), ('one', 1), ('to', 13), ('65', 1), ('opened', 1), ('unsure', 1), ('programs', 2), ('road', 1), ('54', 1), ('', 1), ('files', 2), ('have', 1), ('work', 2), ('country', 2), ('disc', 2), ('started', 1), ('payment', 3), ('102', 1), ('england', 1), ('isp', 2), ('own', 2), ('4', 1), ('of', 4), ('do', 1), ('purchase', 1), ('below', 1), ('form', 1), ('fill', 1), ('friendly', 1), ('normally', 1), ('new', 1), ('ask', 2), ('documents', 1), ('free', 1), ('sent', 1), ('addresses', 12), ('days', 1), ('name', 1), ('8', 1), ('cd', 2), ('type', 1), ('bank', 1), ('only', 1), ('are', 5), ('but', 1), ('i', 1), ('9', 1), ('us', 3), ('mailing', 2), ('other', 1), ('just', 1), ('1xt', 1), ('tick', 2), ('3xd', 1), ('details', 2), ('calcuated', 1), ('them', 1), ('quoted', 1), ('remove', 1), ('000', 18), ('1', 1), ('in', 6), ('subject', 1), ('xx', 1), ('amount', 2), ('as', 4), ('into', 1), ('camberley', 1), ('447', 1), ('rates', 1), ('against', 1), ('usefull', 1), ('_', 251), ('is', 2), ('imported', 1), ('out', 3), ('best', 2), ('163', 1), ('29', 1), ('prophoto', 2), ('related', 1), ('enter', 1), ('value', 1), ('zip', 3), ('mailer', 1), ('an', 1), ('results', 1), ('send', 3), ('84', 1)], 647))]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "def reGrpLst(fw_c): # we get a nested tuple\n",
    "    fw,c = fw_c\n",
    "    f,w = fw\n",
    "    return (f,[(w,c)]) # return (f,[(w,c)]) structure. Can be used verbatim, if your variable names match.\n",
    " \n",
    "def make_f_tfLn_RDD(argDir): \n",
    "    \n",
    "    fw_RDD = read_fw_RDD(argDir)\n",
    "    \n",
    "    #<<< change (f,w) to ((f,w),1)\n",
    "    fw_1_RDD = fw_RDD.map(lambda fw: (fw, 1))\n",
    "    fw_c_RDD = fw_1_RDD.reduceByKey(add) \n",
    "    f_wcL_RDD = fw_c_RDD.map(reGrpLst) #as above\n",
    "    f_wcL2_RDD = f_wcL_RDD.reduceByKey(add)\n",
    "    \n",
    "    \n",
    "    fw_d_RDD = fw_RDD.map(lambda x: (x[0], 1)).reduceByKey(add)\n",
    "    f_wcLn_RDD = f_wcL2_RDD.join(fw_d_RDD) #this returns (f, ([w,c], [w,c]..., dl))\n",
    "    \n",
    "    #You need to divide c by dl ...\n",
    "    #f_wcLn_RDD = f_wcLn_RDD.map( ...)\n",
    "\n",
    "    return f_wcLn_RDD\n",
    "\n",
    "\n",
    "\n",
    "f_wcLn_RDD = make_f_tfLn_RDD( prefix + 'spam/bare/part1') # for testing\n",
    "print(f_wcLn_RDD.take(1)) # for testing\n",
    "\n",
    "#wcLn = f_wcLn_RDD.take(1)[0][1] # get the first normalised word count list\n",
    "#print(sum([cn for (w,cn) in wcLn])) # the sum of normalised counts should be close to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task d) Creating hashed feature vectors \n",
    "Use the hashing trick to create fixed size TF vectors. (10%)\n",
    "\n",
    "Use the code from the week 2 lecture to create the hash vectors.\n",
    "\n",
    "As before, make it a function and add a short test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read /data/tempstore/spam/bare/part1 files from directory 289\n",
      "file word count histogram\n",
      "([0, 10, 100, 1000, 10000], [0, 0, 107, 177])\n",
      "[('file:/data/tempstore/spam/bare/part1/spmsga129.txt', ([('opened', 1), ('unsure', 1), ('programs', 2), ('road', 1), ('54', 1), ('', 1), ('files', 2), ('have', 1), ('work', 2), ('country', 2), ('disc', 2), ('started', 1), ('payment', 3), ('102', 1), ('england', 1), ('isp', 2), ('own', 2), ('4', 1), ('of', 4), ('do', 1), ('purchase', 1), ('below', 1), ('form', 1), ('fill', 1), ('friendly', 1), ('normally', 1), ('new', 1), ('ask', 2), ('documents', 1), ('free', 1), ('sent', 1), ('addresses', 12), ('days', 1), ('name', 1), ('8', 1), ('cd', 2), ('type', 1), ('bank', 1), ('only', 1), ('are', 5), ('but', 1), ('i', 1), ('9', 1), ('us', 3), ('mailing', 2), ('other', 1), ('just', 1), ('1xt', 1), ('tick', 2), ('3xd', 1), ('details', 2), ('calcuated', 1), ('them', 1), ('quoted', 1), ('remove', 1), ('000', 18), ('1', 1), ('in', 6), ('subject', 1), ('xx', 1), ('amount', 2), ('as', 4), ('into', 1), ('camberley', 1), ('447', 1), ('rates', 1), ('against', 1), ('usefull', 1), ('_', 251), ('is', 2), ('imported', 1), ('out', 3), ('best', 2), ('163', 1), ('29', 1), ('prophoto', 2), ('related', 1), ('enter', 1), ('value', 1), ('zip', 3), ('mailer', 1), ('an', 1), ('results', 1), ('send', 3), ('84', 1), ('please', 1), ('99', 1), ('3', 1), ('a', 3), ('surrey', 1), ('fresh', 1), ('pound', 2), ('supplied', 1), ('po', 1), ('the', 13), ('59', 1), ('about', 1), ('net', 1), ('get', 1), ('post', 2), ('international', 1), ('152', 1), ('80', 1), ('dispatched', 1), ('discs', 1), ('date', 1), ('text', 1), ('box', 1), ('137', 1), ('then', 1), ('5', 2), ('120', 1), ('packing', 1), ('sites', 1), ('15', 1), ('lists', 3), ('how', 1), ('92', 1), ('email', 15), ('either', 1), ('who', 1), ('made', 1), ('data', 1), ('and', 11), ('you', 9), ('enclose', 1), ('number', 1), ('fully', 1), ('address', 3), ('come', 2), ('42', 1), ('it', 1), ('packages', 1), ('178', 1), ('cleared', 1), ('by', 2), ('unsolicited', 1), ('2', 1), ('direct', 1), ('exchange', 1), ('code', 1), ('expensive', 1), ('dollars', 3), ('that', 1), ('payable', 1), ('with', 5), ('gu15', 1), ('dollar', 1), ('they', 1), ('much', 1), ('receive', 1), ('most', 2), ('7', 1), ('uk', 2), ('should', 1), ('from', 2), ('not', 3), ('has', 1), ('or', 2), ('inclusive', 1), ('your', 8), ('software', 2), ('wish', 3), ('sterling', 2), ('minimum', 1), ('all', 2), ('worldwide', 1), ('75', 1), ('6', 1), ('soon', 1), ('what', 2), ('web', 1), ('so', 1), ('available', 1), ('bases', 1), ('bulk', 2), ('outside', 1), ('recieve', 1), ('doman', 1), ('products', 1), ('will', 3), ('cleaned', 1), ('dont', 2), ('travellers', 1), ('can', 1), ('postage', 1), ('for', 3), ('prices', 2), ('program', 1), ('word', 1), ('bypass', 1), ('need', 2), ('if', 3), ('order', 4), ('puchased', 1), ('35', 1), ('irritation', 1), ('cheques', 3), ('processing', 1), ('cheque', 1), ('together', 1), ('currency', 5), ('mail', 3), ('on', 2), ('its', 1), ('many', 1), ('recipient', 1), ('those', 1), ('be', 3), ('one', 1), ('to', 13), ('65', 1)], 647)), ('file:/data/tempstore/spam/bare/part1/3-392msg0.txt', ([('areas', 2), ('registration', 1), ('anonymous', 1), ('uc', 2), ('escol', 3), ('deadline', 1), ('davis', 1), ('linbetty', 1), ('twenty', 1), ('lindryer', 1), ('speakers', 1), ('ny', 1), ('abstract', 1), ('croft', 1), ('subject', 1), ('at', 2), ('an', 1), ('texas', 1), ('lexical', 1), ('685', 1), ('stating', 1), ('4', 2), ('presentation', 1), ('john', 1), ('keren', 1), ('interested', 1), ('papers', 3), ('call', 2), ('we', 1), ('title', 1), ('semantics', 1), ('alberta', 1), ('psycholinguistics', 1), ('especially', 1), ('medical', 1), ('michigan', 1), ('university', 6), ('', 1), ('oregon', 1), ('page', 1), ('following', 1), ('neurolinguistics', 1), ('functional', 1), ('jackendoff', 1), ('are', 2), ('center', 1), ('of', 12), ('theoretical', 1), ('department', 2), ('card', 1), ('discourse', 1), ('sent', 1), ('late', 1), ('2177', 1), ('716', 1), ('10', 1), ('in', 4), ('abstracts', 4), ('va', 1), ('minutes', 1), ('tomlin', 1), ('brandeis', 1), ('housing', 1), ('will', 2), ('buffalo', 3), ('americas', 1), ('92', 3), ('allotted', 1), ('perspective', 1), ('william', 1), ('solicited', 1), ('one', 1), ('toronto', 1), ('to', 1), ('russell', 1), ('indigenous', 1), ('13', 1), ('ohala', 1), ('along', 1), ('636', 2), ('by', 1), ('austin', 1), ('dryer', 1), ('should', 2), ('copies', 1), ('cognitive', 1), ('typology', 1), ('on', 1), ('universals', 1), ('nina', 1), ('with', 1), ('for', 5), ('each', 1), ('the', 7), ('information', 2), ('paper', 2), ('experimental', 1), ('rice', 1), ('any', 1), ('berkeley', 1), ('suny', 2), ('author', 1), ('be', 4), ('linguistics', 4), ('14260', 1), ('woodbury', 1), ('1992', 3), ('a', 1), ('fax', 1), ('languages', 1), ('dronkers', 1), ('hall', 1), ('matthew', 1), ('ray', 1), ('3825', 1), ('brown', 1), ('and', 4), ('from', 1), ('contact', 1), ('received', 1), ('submitting', 1), ('all', 1), ('15', 1), ('analysis', 1), ('receiving', 1), ('invited', 1), ('ubvms', 2), ('betty', 1), ('available', 1), ('martinez', 1), ('or', 2), ('baldy', 1), ('november', 1), ('september', 3), ('anthony', 1), ('bitnet', 2)], 205)), ('file:/data/tempstore/spam/bare/part1/spmsga127.txt', ([('c', 1), ('stopped', 1), ('brand', 1), ('sent', 1), ('visit', 2), ('remove', 1), ('at', 1), ('large', 1), ('line', 1), ('hardcore', 1), ('paragraph', 1), ('technology', 1), ('free', 2), ('no', 1), ('young', 1), ('is', 1), ('in', 4), ('this', 3), ('rooms', 1), ('', 1), ('pictures', 1), ('sender', 1), ('new', 2), ('of', 5), ('www', 1), ('latest', 2), ('1618', 1), ('msn', 1), ('301', 2), ('trial', 2), ('further', 1), ('xxx', 1), ('channels', 1), ('may', 1), ('live', 2), ('transmissions', 1), ('oa', 1), ('s', 1), ('subject', 2), ('compliance', 1), ('site', 2), ('sexxxybodies', 1), ('cost', 1), ('conferencing', 1), ('for', 1), ('reply', 1), ('com', 2), ('word', 1), ('be', 1), ('nuko64', 1), ('bill', 1), ('picture', 1), ('adult', 2), ('to', 3), ('you', 2), ('address', 1), ('mail', 1), ('come', 1), ('e', 1), ('experience', 1), ('membership', 2), ('video', 1), ('the', 6), ('http', 1), ('per', 1), ('and', 1), ('2', 1), ('gallery', 1), ('section', 2), ('dancers', 1), ('by', 2), ('sending', 1), ('please', 1), ('our', 1), ('1000', 1), ('with', 1), ('thousands', 1), ('great', 1), ('message', 1), ('a', 3), ('email', 2)], 115))]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 30, 10.207.1.85): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-3-37240242e93b>\", line 14, in <lambda>\n  File \"<ipython-input-3-37240242e93b>\", line 6, in hashing_vectorizer\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-3-37240242e93b>\", line 14, in <lambda>\n  File \"<ipython-input-3-37240242e93b>\", line 6, in hashing_vectorizer\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-37240242e93b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mf_wVn_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_f_wVn_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_f_tfLn_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_wVn_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_wVn_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-37240242e93b>\u001b[0m in \u001b[0;36mmake_f_wVn_RDD\u001b[0;34m(f_wcLn_RDD, argN)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_wcLn_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mf_wVn_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_wcLn_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf_wc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_wc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhashing_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_wc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_wVn_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf_wVn_RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 30, 10.207.1.85): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-3-37240242e93b>\", line 14, in <lambda>\n  File \"<ipython-input-3-37240242e93b>\", line 6, in hashing_vectorizer\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-3-37240242e93b>\", line 14, in <lambda>\n  File \"<ipython-input-3-37240242e93b>\", line 6, in hashing_vectorizer\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def hashing_vectorizer(word_count_list, N): \n",
    "    # use the code from the lecture\n",
    "    v = [0] * N # create fixed size vector of 0s for word_count in word_count_list:\n",
    "    for word_count in word_count_list:\n",
    "        word,count = word_count_list # unpack tuple\n",
    "        h = hash(word) # get hash value\n",
    "        v[h % N] = v[h % N] + count # add count \n",
    "    return v # return hashed word vector\n",
    "    \n",
    "    \n",
    "def make_f_wVn_RDD(f_wcLn_RDD, argN):\n",
    "    # apply hashing_vectorizer in a lambda, this is only a one-liner\n",
    "    print(f_wcLn_RDD.take(3))\n",
    "    f_wVn_RDD = f_wcLn_RDD.map(lambda f_wc: (f_wc[0],hashing_vectorizer(f_wc[1],argN)))\n",
    "    print(f_wVn_RDD.take(3))\n",
    "    return f_wVn_RDD\n",
    "\n",
    "\n",
    "N=100\n",
    "f_wVn_RDD = make_f_wVn_RDD(make_f_tfLn_RDD(dirPath),N) # for testing\n",
    "print(f_wVn_RDD.take(1)[0][1]) # for testing\n",
    "print( sum(f_wVn_RDD.take(1)[0][1])) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task e) Create Labeled Points\n",
    "\n",
    "Determine whether the file is spam (i.e. the filename contains ’spmsg’) and replace the filename by a 1 (spam) or 0 (ham) accordingly. Use map() to create an RDD of LabeledPoint objects. \n",
    "\n",
    "See here [http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) for an example, and here [http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint](http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) for the `LabeledPoint` documentation. (15%)\n",
    "\n",
    "It's useful to take the RDD with normalised word lists as input. \n",
    "\n",
    "For finding the spam messages use `re.search()` see here[https://docs.python.org/3/library/re.html?highlight=re%20search#re.search](https://docs.python.org/3/library/re.html?highlight=re%20search#re.search) for documentation. Search for 'spmsg' in the filename and check whether the result is `None`. The relevan syntax here is <b>`0 if <yourCondition> else 1`</b>, i.e. 0 if 'spmsg' is not in the filename (not spam) and 1 if it is (it's spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def make_lp_RDD(f_tfLn_RDD,argN):\n",
    "    \n",
    "    lp_RDD = f_wVec_RDD.map(lambda f_wVec: LabeledPoint(1 if re.search(f_wVec[0],'spmsg') is not None else 0,f_wVec[1]))     #<<< make a vector\n",
    "    #<<< detect spam by filename \n",
    "    #<<< transform into LabeledPoint objects\n",
    "    return lp_RDD\n",
    "\n",
    "#lp_RDD = make_lp_RNN(make_f_tfLn_RDD(prefix + 'spam/bare/part1'),100)\n",
    "#print(lp_RDD.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task f) Train a classifier \n",
    "\n",
    "Use the `LabeledPoint` objects to train the `LogisticRegression` and calculate the accuracy of the model on the training set (again, follow this example [http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression](http://spark.apache.org/docs/2.0.0/ml-classification-regression.html#logistic-regression) and here is the documentation [http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS](http://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithLBFGS).  (15%) \n",
    "\n",
    "It's useful to make a function that takes as argument a normalised word list again (because we can later also use it with TF.IDF values).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, NaiveBayes\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "path = prefix + 'spam/stop/part1'\n",
    "\n",
    "N=100\n",
    "def trainModel(f_wcL_RDD,N):\n",
    "    ... #<<< get the training data as LabeledPoint objects.\n",
    "    print('training data items: {}, correct: {}'.format(trainData.count(), correct)) # output raw numbers\n",
    "    print('training accuracy {:.1%}'.format(accuracy)) # and accuracy\n",
    "    return model \n",
    "\n",
    "f_wcLn_RDD = make_f_wcLn_RDD(path) # for testing\n",
    "model = trainModel(f_wcLn_RDD,N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task g) Test the classifier\n",
    "\n",
    "Use the files from \\texttt{.../data/extra/spam/bare/part10} and prepare them like in task~a)-e) (use the function you created in task e) and before. Then use the trained model to predict the label for each vector you have and compare it to the original to test the performance of your classifier. (10\\%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testModel(model,f_wcL_RDD,N):\n",
    "    #<<< like with trainModel, transform the data and evaluate it.\n",
    "    print('test data items: {}, correct:{}'.format(testData.count(),correct))\n",
    "    print('testing accuracy {:.1%}'.format(accuracy))\n",
    "\n",
    "testModel(model,make_f_wcLn_RDD('hdfs://saltdean/data/spam/stop/part10'),N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task h) Run experiments \n",
    "\n",
    "Package the whole classifier training and evaluation in one function. Then apply it to the files from `/data/extra/spam/lemm`, `/data/extra/spam/stop` and `/data/extra/spam/lemm_stop` in addition to `/data/extra/spam/bare`  and evaluate the accuracy of your classifier. \n",
    "\n",
    "Comment on the effect of *lemmatisation* and *stopword removal* on classification accuracy. Further, evaluate the use of larger training sets and the effect of different vector sizes. Print out the results of your experiments in readable form. (20%) \n",
    "\n",
    "You need to create one small fuction that combines tasks f) and g), and then apply it to different datasets sizes, vector sizes, and different preprocessings. \n",
    "\n",
    "The combination of the part1-part9 datasets can be achieved by using 'glob' patterns in the filename ('part[1-9]'). This is a feature of the Hadoop filesystem and not well documented in Spark (or anywhere else). You can find a description of its Python implementation here: [https://docs.python.org/3/library/glob.html](https://docs.python.org/3/library/glob.html). You can also supply multiple comma-separated paths, but you'll need to test what works, when you use this feature. Recursive patterns don't seem to work.\n",
    "\n",
    "Alternatively, you can create unions of RDDs for each part. However, this seems to lead to slower execution. With the latter, it is useful to created arrays of the directory names (part1, ...). When you work with unions, it may be useful to start with an empty RDD. That can be created with `sc.parallelize([])`.\n",
    "\n",
    "A useful tool for creatng multiple long paths variants is the use of the Python string format() method as used below. There is a good set of example here: [https://docs.python.org/3/library/string.html#format-examples](https://docs.python.org/3/library/string.html#format-examples) and the specification is here: [https://docs.python.org/3/library/string.html#format-specification-mini-language](https://docs.python.org/3/library/string.html#format-specification-mini-language).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this function combines tasks f) and g)\n",
    "def trainTestModel(trainPaths,testPath,N):\n",
    "    ... #<<< just combine training ans testing here\n",
    "    \n",
    "# prepare the part directories and the path\n",
    "dirPattern = 'hdfs://saltdean/data/spam/bare/part[1-{}]' # the {} can be filled by 'dirPattern.format(i)' \n",
    "# create the path for the test set\n",
    "testPath = 'hdfs://saltdean/data/spam/bare/part10'\n",
    "\n",
    "print('EXPERIMENT 1: Testing different training set sizes')\n",
    "print('Path = {}, N = {}'.format(dirPattern,N)) # using format to make sure we record the parameters of the experiment\n",
    "#<<< make the test set, it will be constant for this experiment\n",
    "#<<< loop over i the number of parts for training (1-9)\n",
    "    trainPath = dirPattern.format(i) # in the loop you can create a path like this\n",
    "    print(trainPath) #just for testing, remove later\n",
    "    #<<< create the trainRDD (using your make_f_tfLn_RDD method)\n",
    "    trainTestModel(trainPaths,testPath,N)\n",
    "\n",
    "print('\\nEXPERIMENT 2: Testing different vector sizes')\n",
    "#<<< loop over different values for N. 3,10,30,100,300, ... is a good pattern\n",
    "    print('=== N = ',N)\n",
    "    trainTestModel(trainPaths,testPath,N)\n",
    "\n",
    "N = 100 # change to what you feel is a good compromise between computation and accuracy\n",
    "# the dictionary below helps associate description and paths.\n",
    "setDict = {'No preprocessing': prefix + 'spam/bare/',\n",
    "           'Stopwords removed': prefix + 'spam/stop/',\n",
    "           'Lemmatised': prefix + 'spam/lemm/',\n",
    "           'Lemmatised and stopwords removed': prefix + 'spam/lemm_stop/'}\n",
    "\n",
    "print('\\nEXPERIMENT 3: Testing differently preprocessed data sets')\n",
    "print('training on parts 1-9, N = {}'.format(N))\n",
    "for sp in setDict:\n",
    "    print('=== ',sp)\n",
    "    trainPath = setDict[sp] + 'part[1-9]'\n",
    "    #<<< make the test (part1-9) and training data (part10) RDDs and evaluate \n",
    "\n",
    "print('\\n====== Done ======')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's an exampe what the output could look like\n",
    "\n",
    "Experiment 1: Testing differddent training set sizes\n",
    "Path = hdfs://saltdean/data/spam/lemm_stop/, N = 1000\n",
    "=== add part  1\n",
    "training data items: 289, correct: 289\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:270\n",
    "testing accuracy 92.8%\n",
    "=== add part  2\n",
    ".\n",
    ".\n",
    ".\n",
    "=== add part  9\n",
    "training data items: 2602, correct: 2602\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:285\n",
    "testing accuracy 97.9%\n",
    "\n",
    "Experiment 2: Testing different vector sizes\n",
    "Path = hdfs://saltdean/data/spam/lemm_stop/, training on parts1-9\n",
    "=== N =  3\n",
    "training data items: 2602, correct: 2171\n",
    "training accuracy 83.4%\n",
    "test data items: 291, correct:237\n",
    "testing accuracy 81.4%\n",
    "=== N =  10\n",
    ".\n",
    ".\n",
    ".\n",
    "=== N =  10000\n",
    "training data items: 2602, correct: 2602\n",
    "training accuracy 100.0%\n",
    "test data items: 291, correct:284\n",
    "testing accuracy 97.6%\n",
    "\n",
    "Experiment 3: Testing differently processed data sets\n",
    "training on parts 1-9, N = 100\n",
    "===  No proprocessing\n",
    ".\n",
    ".\n",
    ".\n",
    "====== Done ======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task i) (Task for pairs) TF.IDF vectors\n",
    "You need to address this task if you are working as a pair. \n",
    "\n",
    "Calculate the IDF values for each word and generate fixed size TF.IDF vectors for each document (word frequencies still normalised by total document word count). Also evaluate the use of TF.IDF compared to normalised word counts in terms of accuracy. (25%)\n",
    "\n",
    "To calculate the IDF values you need to create an RDD (w,f) pairs. You can use the function `RDD.distinct()` to remove duplicates and reorganise to create (w,[f, ...]) lists. The length of the list is the document frequency and can be used to calculate the IDF.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from math import log\n",
    "\n",
    "trainPath = prefix + 'spam/lemm_stop/part[1-9]'\n",
    "testPath = prefix + 'spam/lemm_stop/part10'\n",
    "\n",
    "def make_f_wtfiL_RDD(path):\n",
    "    # Calculuate the IDFs\n",
    "    fw_RDD = read_fw_RDD(path)\n",
    "    #<<< keep only unique (f,w) pairs\n",
    "    #<<< (f,w) -> (w,[f])\n",
    "    #<<< join the lists of files with reduceByKey\n",
    "    vocSize = wfL_RDD.count() # calculate the vocabulary size\n",
    "    print('vocSize: {}'.format(vocSize)) \n",
    "    # calculate the IDF values per word by using len() on the list of files\n",
    "    print('wIdf_RDD.count(): ',wIdf_RDD.count()) # for testing\n",
    "    print(wIdf_RDD.take(2)) # for testing\n",
    "\n",
    "    # Get the normalise word counts (TFs) and organise by word (w,(f,cn))\n",
    "    f_wcLn_RDD = make_f_wcLn_RDD(path) # create the normalised word count lists \n",
    "    #print('f_wcLn_RDD: ',f_wcLn_RDD.map(\n",
    "    #        lambda x: sum([c for (w,c) in x[1]]).histogram([0,10,100,1000,10000]))) # check for the per-file word counts\n",
    "    #<<<< create a list of tuples [(w,(f,cn)), ..] and use flatmap \n",
    "    print('w_fcn_RDD.count(): {}'.format(w_fcn_RDD.count())) # for testing\n",
    "    print(w_fcn_RDD.take(2)) # for testing\n",
    "\n",
    "    # now we can join the IFDs and TFs by the words (w,(f,cn)) join (w,idf) to (w,((f,cn),idf))\n",
    "    #<<< use RDD.join()\n",
    "    print( 'w_fcnIdf_RDD.count(): ', w_fcnIdf_RDD.count())\n",
    "    print( w_fcnIdf_RDD.take(2))\n",
    "\n",
    "    # we have doubly nested tuples (w,((f,cn),idf)) in the RDD, \n",
    "    # but they let us calculate the TF.IDF per file and word (f,[(w,cn*idf)]).\n",
    "    #<<<< map to (f,[(w,cn*idf)])\n",
    "    print('f_wtfiL_RDD.count()', f_wtfiL_RDD.count())\n",
    "    print(str(f_wtfiL_RDD.take(2)))\n",
    "\n",
    "    # with that we can reduce by key (files) to get [(w,tfidf), ...] lists per file.\n",
    "    #<<< reduceByKey\n",
    "    print('# of files with TF.IDF vectors: {}'.format(f_wtfiL2_RDD.count()))\n",
    "    print(f_wtfiL2_RDD.take(2)))\n",
    "\n",
    "    return f_wtfiL2_RDD\n",
    "\n",
    "\n",
    "N=100 # choose a value yourself \n",
    "print('N: {}, trainPath: {}'.format(N,trainPath))\n",
    "#<<< you can now apply trainModel and test Model to RDDs created with make_f_wtfiL_RDD()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Appendix\n",
    "This code is just needed if you have an error message \"sc undefined\". In that case run the code below and try again. You will have to run all code cells from the beginning again, as the new context has no information about what happened before.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# try this in case of \"sc undefined\" errors\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "try: \n",
    "    sc.stop()\n",
    "    print('Stopped existing SparkContext')\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "\n",
    "try: \n",
    "    sc = SparkContext(appName='Coursework part 1')\n",
    "    print('Created new SparkContext')\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "print('Proterties of sc: ',list(sc.getConf().getAll()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
